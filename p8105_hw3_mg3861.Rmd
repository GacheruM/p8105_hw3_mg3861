---
title: "Homework 3 - Visualization"
name: "Margaret Gacheru"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(dplyr)
library(forcats)

```

#Problem 1

First, do some data cleaning by using appropriate variable names, filtering the data based on the “Overall Health” topic, including only responses from “Excellent” to “Poor”, and organizing responses as a factor taking levels ordered from “Excellent” to “Poor”

```{r}
library(p8105.datasets)

brfss = brfss_smart2010%>%
  janitor::clean_names()%>%
  filter(topic == "Overall Health", 
         response %in% c("Excellent", "Very good", "Good", "Fair", "Poor"))%>%
  mutate(response = forcats::fct_relevel(response, "Excellent", "Very good", "Good", "Fair", "Poor"))
         
```


After the data cleaning, begin some exploration:

1. In 2002, we find that Connecticut, Florida, and North Carolina had observations at 7 different locations. Pennsylvania had observations from  the most locations (10),  while 13 states had observations from only 1 location. 

```{r}
filter(brfss, year == 2002)%>%
  distinct(locationabbr, locationdesc)%>%
  count(locationabbr)%>%
  filter(n == 7)%>%
  pull(locationabbr)
```

2. Make a “spaghetti plot” that shows the number of observations in each state from 2002 to 2010.

```{r}

brfss%>%
  group_by(year)%>%
  distinct(locationabbr, locationdesc)%>%
  count(locationabbr)%>%
  ggplot(aes(x = year, y = n, color = locationabbr)) +
  geom_point() +
  geom_line() +
  labs(
    title = "Number of Observations in each state from 2002 to 2010",
    x = "Year",
    y = "Number of Observations"
  )+
  viridis::scale_color_viridis(
    discrete = TRUE
  ) + 
  theme(legend.position="none")

```


3. Make a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State.

There is a decrease in mean proportion of "Excellent" responses from 2002 to 2006 and a slight increase in mean proportion from 2006 and 2010. Furthermoe, the standard deviation of the proportion of "Excellent" responses has decreased across the years. A hypothesis test would be needed to evaluate whether the changes are truly significant.

```{r}

brfss%>%
  filter(locationabbr == "NY", year %in% c(2002, 2006, 2010))%>%
  select(year, locationabbr, locationdesc, response, data_value)%>%
  spread(key = response, value = data_value)%>%
  group_by(year)%>%
  summarize(Mean = mean(Excellent), SD = sd(Excellent))%>%
  knitr::kable(col.names = c("Year", "Mean", "SD"))
  

```

4. For each year and state, compute the average proportion in each response category. Then make a five-panel plot that shows, for each response category separately, the distribution of these state-level averages over time.

On average, the "very good" category has the highest proportions in comparison to the other response groups. The "poor" response category has the lower proportions, on average. There appears to be no overlap between "excellent" and "poor" categories, "fair" and "good" categories, "fair" and "very good" categories, "good" and "poor" categories, as well as "poor" and "very good" categories. Once again, hypothesis testing would be most appropriate to determine whether the differences in the response groups are truly significant.

```{r warning = FALSE}
average = brfss%>%
  select(year, locationabbr, locationdesc, response, data_value)%>%
  spread(key = response, value = data_value)%>%
  janitor::clean_names()%>%
  group_by(year, locationabbr)%>%
  summarize(Excellent = mean(excellent),
            Very_good = mean(very_good),
            Good = mean(good),
            Fair = mean(fair),
            Poor = mean(poor)
            )%>%
  gather(key = response, value = proportion, Excellent:Poor)%>%
  mutate(response = forcats::fct_relevel(response, "Excellent", "Very_good", "Good", "Fair", "Poor"))

average%>%
  ggplot(aes(x = year, y = proportion, color = locationabbr)) +
  geom_line()+
  facet_grid(~response ) +
  scale_x_continuous(breaks = c(2002:2010))+
  viridis::scale_color_viridis(
    discrete = TRUE
  ) + 
  theme(legend.position="none",
        axis.text.x = element_text(angle = 90, hjust = 0))

average%>%
  ggplot(aes(x = as.character(year), y = proportion))+
  geom_boxplot()+
  facet_grid(~response)+
  theme(
        axis.text.x = element_text(angle = 90, hjust = 0))

```

#Problem 2

Using the Instacart data, perform some data cleaning and exploration:

```{r include = FALSE}
library(p8105.datasets)

instacart_data = instacart

instacart_data = instacart_data%>%
  janitor::clean_names()%>%
  mutate(order_dow = recode(as.character(order_dow), "0" = "Sunday",
                            "1" = "Monday",
                            "2" = "Tuesday",
                            "3" = "Wednesday",
                            "4" = "Thursday",
                            "5" = "Friday",
                            "6" = "Saturday"))
```

Write a short description of the dataset, noting the size and structure of the data, describing some key variables, and giving illstrative examples of observations

1. There are 134 aisles. The aisles with the most items ordered include: fresh vegetables, fresh fruit, packaged vegetables and fruits, yogurt, packaged cheese, water/sparkling water. The fresh vegetables and fruits aisles have a disproportionately higher number of orders than the other top 6 aisles (over 150,000 orders in comparison to less than 80,000 orders).  

```{r}
aisles=instacart_data%>%
  group_by(aisle)%>%
  summarize(N = n())
  
aisles%>%
  nrow()

aisles%>%
  arrange(desc(N))%>%
  head(6)%>%
  knitr::kable(col.names = c("Aisle","Number of Orders"))
```

2. Make a plot that shows the number of items ordered in each aisle. Order aisles sensibly, and organize your plot so others can read it

```{r}
aisles%>%
  ggplot(aes(reorder(aisle, desc(N)), N), color = aisle)+
  geom_col()+
  viridis::scale_color_viridis(
    discrete = TRUE) + 
  theme(legend.position = "none",
        axis.text.x = element_text(angle = 90, hjust = 1))

```

3. Make a table showing the most popular item in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”

```{r}
instacart_data%>%
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits"))%>%
  group_by(aisle, product_name)%>%
  summarize(n = n())%>%
  group_by(aisle)%>%
  filter(n == max(n))%>%
  knitr::kable(col.names = c("Aisle", "Item", "Number of Orders"))
```

4. Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week

```{r}
instacart_data%>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream"))%>%
  group_by(product_name, order_dow)%>%
  summarize(Mean_hour = mean(order_hour_of_day))%>%
  spread(key = order_dow, value = Mean_hour)%>%
  knitr::kable(col.names = c("Product Name", "Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"))
```


#Problem 3

Load the NOAA dataset:

```{r}
library(p8105.datasets)

noaa_data = ny_noaa
```

Perform some data cleaning: 

Create separate variables for year, month, and day. 

```{r}
noaa_data = noaa_data%>%
  janitor::clean_names()%>%
  separate(date, into = c("year", "month", "day"), sep = "-")
```

Ensure observations for temperature, precipitation, and snowfall are given in reasonable units. 

```{r}
noaa_data = noaa_data%>%
  mutate(prcp_in = prcp/(10*25.4), 
         snow_in = snow/25.4,
         snwd_in = snwd/(10*25.4),
         tmax_f = ((as.double(tmax)/10)*9/5)+32,
         tmin_f = ((as.double(tmin)/10)*9/5)+32
         )

```

For snowfall, what are the most commonly observed values? Why?

```{r}
noaa_data%>%
  filter(!is.na(snow_in))%>%
  group_by(snow_in)%>%
  summarize(N=n())%>%
  arrange(desc(N))%>%
  head(10)%>%
  knitr::kable(col.names = c("Snow (Inches)", "Number of Observations"))


```

Make a two-panel plot showing the average max temperature in January and in July in each station across years. Is there any observable structure? Any outliers?

```{r}
visual = noaa_data%>%
  select(id, year, month, tmax_f)%>%
  filter(!is.na(tmax_f), month %in% c("01", "07"))%>%
  mutate(month = recode(month, "01" = "January",
                        "07" = "July"))%>%
  group_by(id, month, year)%>%
  summarize(Mean = mean(tmax_f))%>%
  mutate(year = as.integer(year))

visual%>%
  ggplot(aes(x = year, y = Mean, color = id))+
  geom_point()+
  geom_line()+
  facet_grid(~month)+
  viridis::scale_fill_viridis()+
  scale_x_continuous(breaks = c(1981:2010))+
  theme(legend.position = "none",
        axis.text.x = element_text(angle = 90, hjust = 0))

visual %>%
  ggplot(aes(x = as.character(year), y = Mean))+
  geom_boxplot()+
  facet_grid(~month)+
  theme(axis.text.x = element_text(angle = 90, hjust = 0))

```

Make a two-panel plot showing (i) tmax vs tmin for the full dataset (note that a scatterplot may not be the best option); and (ii) make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.



